<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:fb="https://www.facebook.com/2008/fbml" itemscope="itemscope" itemtype="http://schema.org/Product">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="readme-deploy" content="0a76ad4a2ae68a6f7b480e04f93f1d2d4fab9046">
    <title>Phoenix</title>
    <meta http-equiv="Content-type" content="text/html;charset=UTF-8">
    <meta name="description" content="Productive. Reliable. Fast. A productive web framework that does not compromise speed and maintainability">
    <link href="/favicon.ico" rel="shortcut icon" type="image/x-icon">
    <meta property="og:title" content="Phoenix">
    <meta property="og:description" content="Productive. Reliable. Fast. A productive web framework that does not compromise speed and maintainability">
    <meta property="og:image" content="https://files.readme.io/PKPqx8L9TJSAY2Bnmw7F_phoenixframework-logo-white@2x.png">
    <meta property="og:site_name" content="Phoenix">
    <meta id="config-proxy-url" content="https://proxy.readme.io/proxy">
    <link rel="canonical" href="http://www.phoenixframework.org/www.phoenixframework.org">
    <link rel="alternate" type="application/rss+xml" title="RSS Feed for Phoenix" href="/blog.rss">
    <link rel="stylesheet" href="/assets/css/base.css" />
    <script type="text/javascript" src="/assets/js/app.js"></script>

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-57337100-1', 'auto');
      ga('send', 'pageview');
    </script>
    <link href="//fonts.googleapis.com/css?family=Open+Sans:400:sans-serif|Open+Sans:400:sans-serif" rel="stylesheet" type="text/css">
  </head>

  <body class="layout page-home body-none theme-solid header-solid header-bg-size-auto header-bg-pos-tl header-overlay-triangles lumosity-light undefined theme-threes ng-scope os-linux" cz-shortcut-listen="true">
<div class="wrapper">
  <header id="header" class="header">
    <div class="container">
      <h1 class="navbar-brand"><a href="/">Phoenix</a></h1>
      <div class="navbar-collapse collapse">
        <ul class="nav navbar-nav">
          <li><a href="https://hexdocs.pm/phoenix/overview.html">Guides</a></li>
          <li><a href="https://hexdocs.pm/phoenix/Phoenix.html">Docs</a></li>
          <li><a href="/community">Community</a></li>
          <li><a href="https://github.com/phoenixframework/phoenix">GitHub</a></li>
          <li><a href="/blog" title="Blog">Blog</a></li>
        </ul>

        <ul class="nav navbar-nav pull-right">
          <li><a class="phx-book" href="https://pragprog.com/book/phoenix/programming-phoenix" title="Book">üìô &nbsp;&nbsp; Book</a></li>
        </ul>
      </div>
    </div>
    </header>
    <br/>
<div class="container body-container">
  <div class="row">
    <div class="col-sm-3 border-right">
        <div class="sidebar-nav">
          <h4>News</h4>
          <ul>
            <li><a href="/blog/phoenix-1-3-0-released" class="active"><span>Phoenix 1.3.0 Released</span></a></li>
            <li><a href="/blog/contribution-sprint" class="active"><span>Contribution Sprint</span></a></li>
            <li><a href="/blog/upgrading-from-120-to-130" class="active"><span>Upgrading from 1.2.x to 1.3.0</span></a></li>
            <li><a href="/blog/upgrading-from-11x-to-120" class="active"><span>Upgrading from 1.1.x to 1.2.0</span></a></li>
            <li><a href="/blog/upgrading-from-111-to-112" class="active"><span>Upgrading from 1.1.1 to 1.1.2</span></a></li>
            <li><a href="/blog/upgrading-from-v10-to-v11" class="active"><span>Upgrading from v1.0 to v1.1</span></a></li>
            <li><a href="/blog/the-road-to-2-million-websocket-connections" class="active"><span>The Road to 2 Million Websocket Connections in Phoenix</span></a></li>
            <li><a href="/blog/phoenix-10-the-framework-for-the-modern-web-just-landed"><span>Phoenix 1.0 ‚Äì the framework for the modern web just landed</span></a></li>
            <li><a href="/blog/phoenix-0100-released-with-assets-handling-generat"><span>Phoenix 0.10.0 released with assets handling, generators, &amp; more</span></a></li>
          </ul>
        </div>
    </div>
    <div class="col-sm-9 border-left">
      <div class="docs-content">
        <div class="docs-header">
          <h1>The Road to 2 Million Websocket Connections in Phoenix</h1>
          
            <div class="byline">
                <span>By Gary Rennie</span>
                &nbsp;¬∑&nbsp;<span class="ago">2015-11-03</span>
                &nbsp;¬∑&nbsp;<span>v1.0.0</span>
            </div>
          
          <hr>
        </div>
        <div class="content block-content ready">
          <p><img src="/assets/img/blog/2m-ws/top.png" alt=""/></p>
<p>If you have been paying attention on Twitter recently, you have likely seen some increasing numbers regarding the number of simultaneous connections the Phoenix web framework can handle. This post documents some of the techniques used to perform the benchmarks.</p>
<h2>How It Started</h2>
<p>A couple of weeks ago I was trying to benchmark the number of connections and managed to get 1k connections on my local machine. I wasn‚Äôt convinced by the number so I posted in IRC to see if anyone had benchmarked Phoenix channels. It turned out they had not, but some members of the core team found the 1k number I provided suspiciously low. This was the beginning of the journey.</p>
<h2>How To Run The Benchmarks</h2>
<h3>The Server</h3>
<p>To benchmark the number of simultaneous web sockets that can be open at a time, the first thing required is a Phoenix application to accept the sockets. For these tests, we used a slightly modified version of the <a href="https://github.com/chrismccord/phoenix_chat_example">chrismccord/phoenix_chat_application</a> available at <a href="https://github.com/Gazler/phoenix_chat_example/tree/bench">Gazler/phoenix_chat_example</a> - the key difference is:</p>
<ol>
<li>The <code class="inline">after_join</code> hook that broadcasts a user has joined a channel has been removed. When measuring concurrent connections, we want to limit the number of messages that are sent. There will be future benchmarks for that.
</li>
</ol>
<p>Most of these tests were performed on <a href="http://www.rackspace.com/cloud/servers#dd-wrap-iov1">Rackspace 15 GB I/O v1</a> - these machines have 15GB RAM and 4 cores. <a href="https://www.rackspace.com">Rackspace</a> kindly let us use 3 of these servers for our benchmarks free of charge. They also let us use a <a href="http://www.rackspace.com/cloud/servers">OnMetal I/O</a> which had 128GB RAM and showed 40 cores in htop.</p>
<p><img src="/assets/img/blog/2m-ws/htop.png" alt=""/></p>
<p>One additional change you may want to make is to remove <code class="inline">check_origin</code> in <code class="inline">conf/prod.exs</code> - this will mean that the application can be connected to regardless of the ip address/hostname used.</p>
<p>To start the server just <code class="inline">git clone</code> it and run:</p>
<ol>
<li><code class="inline">MIX_ENV=prod mix deps.get</code>
</li>
<li><code class="inline">MIX_ENV=prod mix deps.compile</code>
</li>
<li><code class="inline">MIX_ENV=prod PORT=4000 mix phoenix.server</code>
</li>
</ol>
<p>You can validate this is working by visiting <code class="inline">YOUR_IP_ADDRESS:4000</code></p>
<h3>The Client</h3>
<p>For running the client, we used <a href="http://tsung.erlang-projects.org">Tsung</a>. Tsung is an open-source distributed load testing tool that makes it easy to stress test websockets (as well as many other protocols.)</p>
<p>The way Tsung works when distributing is by using host names. In our example, the first machine was called ‚Äúphoenix1‚Äù which was assigned against the ip in <code class="inline">/etc/hosts</code>. The other machines ‚Äúphoenix2‚Äù and ‚Äúphoenix3‚Äù should also be in <code class="inline">/etc/hosts</code>.</p>
<p>It is important to run the clients on a different machine from the Phoenix application for the benchmarks. The results will not be a true representation if both are running on the same machine.</p>
<p>Tsung is configured using XML files. You can read about the particular values in <a href="http://tsung.erlang-projects.org/user_manual/index.html">the documentation</a>. Here is the config file we used (however the numbers have been lowered to reflect the number of clients here, for our bigger tests we used 43 clients). It starts 1k connections a second up to a maximum of 100k connections. For each connection it opens a websocket, joins the ‚Äúrooms:lobby‚Äù topic and then sleeps for 30000 seconds.</p>
<p>We used a large sleep time because we wanted to keep the connections open to see how responsive the application was after all the clients had connected. We would stop the test manually instead of closing the websockets in the config (Which you can do with <code class="inline">type=&quot;disconnect&quot;</code>).</p>
<pre><code class="xml">&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;!DOCTYPE tsung SYSTEM &quot;/user/share/tsung/tsung-1.0.dtd&quot;&gt;
&lt;tsung loglevel=&quot;debug&quot; version=&quot;1.0&quot;&gt;
  &lt;clients&gt;
    &lt;client host=&quot;phoenix1&quot; cpu=&quot;4&quot; use_controller_vm=&quot;false&quot; maxusers=&quot;64000&quot; /&gt;
    &lt;client host=&quot;phoenix2&quot; cpu=&quot;4&quot; use_controller_vm=&quot;false&quot; maxusers=&quot;64000&quot; /&gt;
    &lt;client host=&quot;phoenix3&quot; cpu=&quot;4&quot; use_controller_vm=&quot;false&quot; maxusers=&quot;64000&quot; /&gt;
  &lt;/clients&gt;

  &lt;servers&gt;
    &lt;server host=&quot;server_ip_address&quot; port=&quot;4000&quot; type=&quot;tcp&quot; /&gt;
  &lt;/servers&gt;

  &lt;load&gt;
    &lt;arrivalphase phase=&quot;1&quot; duration=&quot;100&quot; unit=&quot;second&quot;&gt;
      &lt;users maxnumber=&quot;100000&quot; arrivalrate=&quot;1000&quot; unit=&quot;second&quot; /&gt;
    &lt;/arrivalphase&gt;
  &lt;/load&gt;

  &lt;options&gt;
    &lt;option name=&quot;ports_range&quot; min=&quot;1025&quot; max=&quot;65535&quot;/&gt;
  &lt;/options&gt;

  &lt;sessions&gt;
    &lt;session name=&quot;websocket&quot; probability=&quot;100&quot; type=&quot;ts_websocket&quot;&gt;
      &lt;request&gt;
        &lt;websocket type=&quot;connect&quot; path=&quot;/socket/websocket&quot;&gt;&lt;/websocket&gt;
      &lt;/request&gt;

      &lt;request subst=&quot;true&quot;&gt;
        &lt;websocket type=&quot;message&quot;&gt;{&quot;topic&quot;:&quot;rooms:lobby&quot;, &quot;event&quot;:&quot;phx_join&quot;, &quot;payload&quot;: {&quot;user&quot;:&quot;%%ts_user_server:get_unique_id%%&quot;}, &quot;ref&quot;:&quot;1&quot;}&lt;/websocket&gt;
      &lt;/request&gt;

      &lt;for var=&quot;i&quot; from=&quot;1&quot; to=&quot;1000&quot; incr=&quot;1&quot;&gt;
        &lt;thinktime value=&quot;30&quot;/&gt;
      &lt;/for&gt;
    &lt;/session&gt;
  &lt;/sessions&gt;
&lt;/tsung&gt;</code></pre>
<h2>The First 1k Connections</h2>
<p>Tsung provides a web interface at port <code class="inline">8091</code> which can be used to monitor the status of the test. The only chart that we were really interested in for these tests was similtaneous users. So the first time I ran Tsung on my own was on my own machine with both Tsung and the Phoenix chat application running locally. When doing this Tsung would often crash - when this happens you can‚Äôt see the web interface - which means there is no chart to show for this, but it was an unimpressive 1k connections.</p>
<h2>The First 1k Connections Again!</h2>
<p>I set up a machine remotely and attempted benchmarking again. This time I was getting 1k connections, but at least Tsung didn‚Äôt crash. The reason for this was the system-wide resource limit was being reached. To verify this I ran <code class="inline">ulimit -n</code> which returned <code class="inline">1024</code> which would explain why I could only get 1k connections.</p>
<p>From this point onwards the following configuration was used. This configuration took us all the way to 2 million connections.</p>
<pre><code class="">sysctl -w fs.file-max=12000500
sysctl -w fs.nr_open=20000500
ulimit -n 20000000
sysctl -w net.ipv4.tcp_mem=&#39;10000000 10000000 10000000&#39;
sysctl -w net.ipv4.tcp_rmem=&#39;1024 4096 16384&#39;
sysctl -w net.ipv4.tcp_wmem=&#39;1024 4096 16384&#39;
sysctl -w net.core.rmem_max=16384
sysctl -w net.core.wmem_max=16384</code></pre>
<h2>The First Real Benchmark</h2>
<p>I had been talking about Tsung in IRC when Chris McCord (creator of Phoenix) contacted me to let me know RackSpace had set up some instances for us to use for the benchmarks. We got to work setting up the 3 servers with the following config file: <a href="https://gist.github.com/Gazler/c539b7ef443a6ea5a182"><a href="https://gist.github.com/Gazler/c539b7ef443a6ea5a182">https://gist.github.com/Gazler/c539b7ef443a6ea5a182</a></a></p>
<p>After we were up and running we dedicated one machine to Phoenix and two for running Tsung. Our first real benchmark ended up with about 27k connections.</p>
<p><img src="/assets/img/blog/2m-ws/30k.png" alt=""/></p>
<p>In the above image there are two lines on the chart, the line on the top is labeled ‚Äúusers‚Äù and the line on the bottom labeled ‚Äúconnected‚Äù. The users increases based on arrival rate. For most of these tests we used an arrival rate of 1000 users per second.</p>
<p>As soon as the results were in, Jos√© Valim was on the case with <a href="https://github.com/phoenixframework/phoenix/commit/061c69b43c3b8c6fa19fd129d35a3a25ae767850">this commit</a></p>
<p>This was our first improvement and it was a big one. From this we got up to about 50k connections.</p>
<p><img src="/assets/img/blog/2m-ws/50k.png" alt=""/></p>
<h2>Observing the Changes</h2>
<p>After our first improvement we realized that we were going in blind. If only there was some way we could observe what was happening. Luckily for use Erlang ships with <a href="http://www.erlang.org/doc/apps/observer/observer_ug.html">observer</a> and it can be used remotely. We used the following technique from <a href="https://gist.github.com/pnc/9e957e17d4f9c6c81294"><a href="https://gist.github.com/pnc/9e957e17d4f9c6c81294">https://gist.github.com/pnc/9e957e17d4f9c6c81294</a></a> to open a remote observer.</p>
<p><img src="/assets/img/blog/2m-ws/observer.png" alt=""/></p>
<p>Chris was able to use the observer to order the processes by the size of their mailbox. The <code class="inline">:timer</code> process had about 40k messages in its mailbox. This is due to Phoenix doing a heartbeat every 30 seconds to ensure the client is still connected.</p>
<p>Luckily, Cowboy already takes care of this, so after <a href="https://github.com/phoenixframework/phoenix/commit/7b252f42cc8552496a5ebd392f59a008ef6c98a9">this commit</a> the results looked like:</p>
<p><img src="/assets/img/blog/2m-ws/100k.png" alt=""/></p>
<p>I actually killed the pubsub supervisor using observer in this image which explains the 100k drop at the end. This was the second 2x performance gain. The result was 100k concurrent connections using 2 Tsung machines.</p>
<h2>We Need More Machines</h2>
<p>There are two problems with the image above. One is that we don‚Äôt reach the full number of clients (about 15k were timing out) and two we can only actually generate between 40k and 60k connections per Tsung client (technically per IP address.) For Chris and I this wasn‚Äôt good enough. We couldn‚Äôt really see the limits unless we could generate more load.</p>
<p>At this stage RackSpace had given us the 128GB box, so we actually had another machine we could use, using such a powerful machine as a Tsung client limited to 60k connections may seem like a waste, but it‚Äôs better than the machine idling! Chris and I set up another 5 boxes between us which is another 300k possible connections.</p>
<p>We ran the benchmarks again and we got about 330k connected clients.</p>
<p><img src="/assets/img/blog/2m-ws/16gb-330k.png" alt=""/></p>
<p>The big problem is about 70k didn‚Äôt actually connect to the machine. We couldn‚Äôt work out why. Probably hardware issues. We decided to try running Phoenix on the 128GB machine instead. Surely there would be no issues reaching our connection limits, right?</p>
<p><img src="/assets/img/blog/2m-ws/128gb-347k.png" alt=""/></p>
<p>Wrong. The results here are almost identical to those above. Chris and I thought 330k was pretty good. Chris tweeted out the results and we called it a night.</p>
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Calling it quits trying to max Channels‚Äì at 333k clients. It took maxed ports on 8 servers to push that, 40% mem left. We‚Äôre out of servers!</p>&mdash; Chris McCord (@chris_mccord) <a href="https://twitter.com/chris_mccord/status/657716607578472448">October 24, 2015</a></blockquote> <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script><h2>Know Your ETS Types</h2>
<p>After achieving 330k and having 2 fairly easy performance gains, we weren‚Äôt sure there could be any more performance gains of the same magnitude. We were wrong. I wasn‚Äôt aware of it at the time, but my colleague Gabi Zuniga (<a href="https://twitter.com/gabiz">@gabiz</a>) at <a href="http://voicelayer.io">VoiceLayer</a> had been looking at the issue over the weekend. His commit gave us the best performance gain so far. You can see the diff on <a href="https://github.com/phoenixframework/phoenix/pull/1311">the pull request</a>. I‚Äôll also provide it here for convenience:</p>
<pre><code class="diff">-    ^local = :ets.new(local, [:bag, :named_table, :public,
+    ^local = :ets.new(local, [:duplicate_bag, :named_table, :public,</code></pre>
<p>Those 10 additional characters made the chart look like this:</p>
<p><img src="/assets/img/blog/2m-ws/420k.png" alt=""/></p>
<p>Not only did it increase the number of concurrent connections. It also allowed us to increase the arrival rate 10x too. Which made subsequent tests much faster.</p>
<p>The difference between <code class="inline">bag</code> and <code class="inline">duplicate_bag</code> is that <code class="inline">duplicate_bag</code> will allow multiple entries for the same key. Since each socket can only connect once and have one pid, using a duplicate bag didn‚Äôt cause any issues for us.</p>
<p>This maxed out at around 450k connections. At this point the 16GB box was out of memory. We were now ready to really test the larger box.</p>
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">I called it quits too early on the channel bench‚Äôs, with an optimization by <a href="https://twitter.com/gabiz">@gabiz</a> , we have now maxed our 4core/15gb box @ 450k clients!</p>&mdash; Chris McCord (@chris_mccord) <a href="https://twitter.com/chris_mccord/status/658393399821795328">October 25, 2015</a></blockquote> <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script><h2>We Need Even More Machines</h2>
<p>Justin Schneck (<a href="https://twitter.com/mobileoverlord">@mobileoverlord</a>) informed us on IRC that he and his company <a href="http://www.livehelpnow.net/">Live Help Now</a> would set up some additional servers on RackSpace for us to use. 45 additional servers to be precise.</p>
<p>We set up a few machines and set the threshold for Tsung to be 1 million connections. Which was a new milestone that was easily achieved by the 128GB machine:</p>
<p><img src="/assets/img/blog/2m-ws/1m.png" alt=""/></p>
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">On a bigger <a href="https://twitter.com/Rackspace">@Rackspace</a> box we just 1 million phoenix channel clients on a single server! Quick screencast in action:<a href="https://t.co/ONQcVWWdy1">https://t.co/ONQcVWWdy1</a></p>&mdash; Chris McCord (@chris_mccord) <a href="https://twitter.com/chris_mccord/status/658767562231185408">October 26, 2015</a></blockquote> <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script><p>By the time Justin finished setting up all 45 boxes we were convinced 2 million connections was possible. Unfortunately that wasn‚Äôt the case. There was a new bottleneck that only started appearing at 1.3 million connections!</p>
<p><img src="/assets/img/blog/2m-ws/1.3m.png" alt=""/></p>
<p>That was it. 1.3M connections is good enough, right? Wrong. At the same time we hit 1.3M subscribers, we started getting regular timeouts when asking to subscribe to the single pubsub server. We also notice a large increase in broadcast time, taking over 5s to broadcast to all subscribers.</p>
<p>Justin is interested in Internet of (useful) Things, and wanted to see if we could optimize broadcasts for 1.3+M subscribers since he sees real use cases at these levels. He had the idea to shard broadcasts by chunking the subscribers and parellizing the broadcast work. We trialed this idea and it reduced the broadcast time back down to 1-2s. But, we still had those pesky subscribe timeouts. We were at the limits of a single pubsub server and single ets table. So Chris started work to pool the pubsub servers, and we realized we could combine Justin‚Äôs broadcast sharding with a pool of pubsub servers and ets tables. So we sharded by subscriber pid into a pool of pubsub servers each managing their own ets table per shard. This let us to reach 2M subscribers without timeouts and maintain 1s broadcasts. The change is on <a href="https://github.com/phoenixframework/phoenix/compare/c114704...cm-local-pool">this commit</a> which is being refined before merging in to master.</p>
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Final results from Phoenix channel benchmarks on 40core/128gb box. 2 million clients, limited by ulimit<a href="https://twitter.com/hashtag/elixirlang?src=hash">#elixirlang</a> <a href="https://t.co/6wRUIfFyKZ">pic.twitter.com/6wRUIfFyKZ</a></p>&mdash; Chris McCord (@chris_mccord) <a href="https://twitter.com/chris_mccord/status/659430661942550528">October 28, 2015</a></blockquote> <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script><p>So there you have it. 2 million connections! Each time we thought there were no more optimizations to be made, another idea was pitched leading to a huge improvement in performance.</p>
<p>2 million is a figure we are pleased with. However, we did not quite max out the machine and we have not yet made any effort toward reducing the memory usage of each socket handler. In addition, there are more benchmarks we will be performing. This particular set of benchmarks was set exclusively around the number of simultaneous open sockets. A chat room with 2 million users is awesome, especially when the messages are broadcast so quickly. This is not a typical use case though. Here are some future benchmarking ideas:</p>
<ul>
<li>One channel with x users sending y messages
</li>
<li>X channels with 1000 users sending y messages
</li>
<li>Running the Phoenix application across multiple nodes
</li>
<li>A simulation sending a random number of messages with users arriving and leaving randomly to behave like a real chat room
</li>
</ul>
<p>The improvements discovered during this benchmark test will be made available in an upcoming release of Phoenix. Keep an eye out for information on future benchmark tests where Phoenix will continue to push the boundaries of the modern web.</p>

        </div>
      </div>
    </div>
  </div>
</div>

  </div>
  </body>
</html>
